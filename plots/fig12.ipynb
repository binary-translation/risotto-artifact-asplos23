{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ddb186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sbs\n",
    "import matplotlib.ticker as mtick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37ad79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script parameters\n",
    "input_csv = \"../results/parsec-phoenix.csv\"\n",
    "baseline = \"x86_64,qemu,qemu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d3595c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing baseline argument\n",
    "base_arch, base_runtime, base_tag = baseline.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef9702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input file\n",
    "df = pd.read_csv(input_csv, sep=';')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b68ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract baseline from dataframe\n",
    "base_df = df.loc[(df['arch'] == base_arch) & (df['runtime'] == base_runtime) & (df['tag'] == base_tag)]\n",
    "base_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617134c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mean for each baseline benchmark\n",
    "base_means = {}\n",
    "for b in set(base_df['bench']):\n",
    "    base_means[b] = np.array(base_df.loc[base_df['bench'] == b]['value'].values, dtype=np.float32).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3734a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the mean of every benchmark for each runtime\n",
    "mean_df = pd.DataFrame()\n",
    "for b in sorted(set(df['bench'])):\n",
    "    df_b = df.loc[df['bench'] == b]\n",
    "    tmp_dict = { 'bench': b }\n",
    "    for t in set(df_b['tag']):\n",
    "        df_b_t = df_b.loc[df_b['tag'] == t]\n",
    "        tmp_dict[t] = np.mean(df_b_t['value'])\n",
    "    mean_df = mean_df.append(tmp_dict, ignore_index=True)\n",
    "mean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a2ded4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize all results from original df to these means\n",
    "df_norm = pd.DataFrame(columns=['arch', 'bench', 'dataset', 'threads', 'unit', 'value', 'runtime',\n",
    "                                'tag', 'norm', 'label'])\n",
    "norm_vals = []\n",
    "for row in df.itertuples():\n",
    "    try:\n",
    "        if row.arch == base_arch and row.runtime == base_runtime and row.tag == base_tag:\n",
    "            continue\n",
    "        # norm = base_means[row.bench] / float(row.value)      # speedup\n",
    "        norm = 100 * float(row.value) / base_means[row.bench]    # relative perf\n",
    "        \n",
    "        # norm = 100 * (base_means[row.bench] - float(row.value)) / base_means[row.bench]\n",
    "        dct = row._asdict()\n",
    "        dct['norm'] = norm\n",
    "        dct['label'] = f\"{dct['tag']}\"\n",
    "        # dct['label'] = f\"{dct['runtime']}-{dct['tag']}\"\n",
    "        del dct['Index']\n",
    "        del dct['cmdline']\n",
    "        norm_vals.append(dct)\n",
    "    except KeyError:\n",
    "        pass\n",
    "df_norm = df_norm.append(norm_vals, ignore_index=True)\n",
    "df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd9c1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor xticks to remove benchmark suite prefix\n",
    "xlabels = []\n",
    "xticks = []\n",
    "for idx, b in enumerate(sorted(set(df_norm['bench']))):\n",
    "    if b.startswith(\"parsec.\"):\n",
    "        xlabels.append(b[7:])\n",
    "        xticks.append(idx)\n",
    "    else:\n",
    "        xlabels.append(b[8:])\n",
    "        xticks.append(idx)\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=(10, 3), dpi=500)\n",
    "sbs.set(style=\"whitegrid\")\n",
    "palette = {\n",
    "    'orange': '#faa200',\n",
    "    'sky blue': '#00b7ec',\n",
    "    'bluish green': '#00a077',\n",
    "    'yellow': '#f5e636',\n",
    "    'blue': '#0077b8',\n",
    "    'vermillion': '#f3640d',\n",
    "    'reddish purple': '#e47ead'\n",
    "}\n",
    "ax = sbs.barplot(data=df_norm, ci='sd',\n",
    "                 x='bench', y='norm',\n",
    "                 hue='label',# palette=palette,\n",
    "                 order=sorted(set(df_norm['bench'])), hue_order=['no-fences', 'tcg-tso', 'risotto', 'native'])\n",
    "plt.grid(visible=True, axis='y')\n",
    "plt.xticks(ticks=xticks, labels=xlabels, rotation=30, ha=\"right\", fontsize='xx-small')\n",
    "ax.set_axisbelow(True)\n",
    "plt.xlabel(\"\")\n",
    "max_val = max(df_norm['norm'].values)\n",
    "plt.ylim(0, max_val*1.05)\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "plt.ylabel(\"Run time w.r.t. QEMU\")\n",
    "plt.axhline(y=100, xmin=0, xmax=1, color='tomato', linewidth=2.5)\n",
    "# Annotate the raw value of the baseline\n",
    "for idx, value in enumerate(sorted(set(base_means))):\n",
    "    tmp = df_norm.loc[df_norm['bench'] == value]['norm'].values.mean()\n",
    "    v = base_means[value]\n",
    "    if v < 10:\n",
    "        v_str = f\"{v:.1f}\"\n",
    "    else:\n",
    "        v_str = f\"{v:.0f}\"\n",
    "    plt.text(idx, max(tmp, max_val), v_str, fontsize='xx-small', color='tomato', ha='center')\n",
    "    \n",
    "# Set color + hatch\n",
    "style = {\n",
    "    'fill': [ True, True, True, True ],\n",
    "    'color': [ palette['vermillion'], palette['sky blue'], palette['bluish green'], palette['orange'] ],\n",
    "    'hatch': [ '', '///', '', ''],\n",
    "    'label': ['no-fences [incorrect]', 'tcg-ver', 'risotto', 'native'],\n",
    "    'edgecolor': [ 'black', 'black', 'black', 'black' ]\n",
    "}\n",
    "for idx, bar in enumerate(ax.patches):\n",
    "    bar_nr = int(idx / len(base_means))\n",
    "    bar.set(color=style['color'][bar_nr], fill=style['fill'][bar_nr],\n",
    "            hatch=style['hatch'][bar_nr], edgecolor=style['edgecolor'][bar_nr])\n",
    "    \n",
    "# parsec / phoenix separation\n",
    "#plt.vlines([ 8.5 ], ymin=0, ymax=1.2, linestyle='solid', colors='black', linewidth=2.5, zorder=10)\n",
    "#matplotlib.text(.4, .1, \"parsec\", xycoords='axes points')\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "plt.legend(labels=style['label'], handles=handles, loc='upper center', bbox_to_anchor=(0.5, 1.15),\n",
    "           borderaxespad=0, ncol=4, fontsize='x-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea34e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"fig12.pdf\", dpi=500, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2095965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute fence cost\n",
    "fence_cost = 100 * (1 - mean_df['no-fences'] / mean_df['qemu'])\n",
    "print(f\"Average time spent on fences: {fence_cost.mean():.2f}%\")\n",
    "fence_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f83d335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tcg-tso gain\n",
    "for b in sorted(set(df_norm['bench'])):\n",
    "    d = df_norm.loc[df_norm['bench'] == b]\n",
    "    d = d.loc[d['tag'] == 'tcg-tso']\n",
    "    print(f\"{b}: {d['value'].values.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8934693",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_df['tcg-tso-pct'] = 100 * (1 - mean_df['tcg-tso'] / mean_df['qemu'])\n",
    "mean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71c15f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_df['tcg-tso-pct'].values.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
