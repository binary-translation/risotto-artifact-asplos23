{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa92dc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sbs\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae0ee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_csv = \"../results/math.csv\"\n",
    "baseline = \"x86_64,qemu,qemu\"\n",
    "base_arch, base_runtime, base_tag = baseline.split(',')\n",
    "df = pd.read_csv(input_csv, sep=';')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4a1715",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df = df.loc[df['tag'] == 'qemu']\n",
    "base_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe644ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mean for each baseline benchmark\n",
    "base_means = {}\n",
    "for b in set(base_df['bench']):\n",
    "    base_means[b] = np.array(base_df.loc[base_df['bench'] == b]['value'].values, dtype=np.float32).mean()\n",
    "base_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3ce4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the mean of every benchmark for each runtime\n",
    "mean_df = pd.DataFrame()\n",
    "for b in sorted(set(df['bench'])):\n",
    "    df_b = df.loc[df['bench'] == b]\n",
    "    tmp_dict = { 'bench': b }\n",
    "    for t in set(df_b['tag']):\n",
    "        df_b_t = df_b.loc[df_b['tag'] == t]\n",
    "        tmp_dict[t] = np.mean(df_b_t['value'])\n",
    "    mean_df = mean_df.append(tmp_dict, ignore_index=True)\n",
    "mean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d4f8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize all results from original df to these means\n",
    "df_norm = pd.DataFrame(columns=['arch', 'bench', 'dataset', 'threads', 'unit', 'value', 'runtime',\n",
    "                                'tag', 'norm', 'label'])\n",
    "norm_vals = []\n",
    "for row in df.itertuples():\n",
    "    try:\n",
    "        if row.arch == base_arch and row.runtime == base_runtime and row.tag == base_tag:\n",
    "            continue\n",
    "        # norm = base_means[row.bench] / float(row.value)      # speedup\n",
    "        norm = float(row.value) / base_means[row.bench]    # relative perf\n",
    "        \n",
    "        # norm = 100 * (base_means[row.bench] - float(row.value)) / base_means[row.bench]\n",
    "        dct = row._asdict()\n",
    "        dct['norm'] = norm\n",
    "        dct['label'] = f\"{dct['tag']}\"\n",
    "        dct['bench'] = dct['bench'][11:]\n",
    "        # dct['label'] = f\"{dct['runtime']}-{dct['tag']}\"\n",
    "        del dct['Index']\n",
    "        del dct['cmdline']\n",
    "        norm_vals.append(dct)\n",
    "    except KeyError:\n",
    "        pass\n",
    "df_norm = df_norm.append(norm_vals, ignore_index=True)\n",
    "df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4bc8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def sorted_nicely( l ): \n",
    "    \"\"\" Sort the given iterable in the way that humans expect.\"\"\" \n",
    "    convert = lambda text: int(text) if text.isdigit() else text \n",
    "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ] \n",
    "    return sorted(l, key = alphanum_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ee07c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(df_norm['norm'].values)\n",
    "df_norm.loc[df_norm['bench'] == 'log']['norm'].values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99becbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,2.5), dpi=500)\n",
    "sbs.set(style=\"whitegrid\")\n",
    "palette = {\n",
    "    'orange': '#faa200',\n",
    "    'sky blue': '#00b7ec',\n",
    "    'bluish green': '#00a077',\n",
    "    'yellow': '#f5e636',\n",
    "    'blue': '#0077b8',\n",
    "    'vermillion': '#f3640d',\n",
    "    'reddish purple': '#e47ead'\n",
    "}\n",
    "order = [ 'sqrt', 'exp', 'log', 'cos', 'sin', 'tan', 'acos', 'asin', 'atan' ]\n",
    "ax = sbs.barplot(data=df_norm, x='bench', y='norm', hue='label',\n",
    "                 hue_order=['risotto', 'native'], order=order)\n",
    "plt.grid(visible=True, axis='y')\n",
    "plt.xticks(rotation=0, ha=\"center\", fontsize='xx-small')\n",
    "plt.xlabel(\"Function\", fontsize='x-small')\n",
    "plt.ylabel(\"Speedup w.r.t. QEMU\")\n",
    "max_val = max(df_norm['norm'].values)\n",
    "plt.ylim(0, max_val*1.1)\n",
    "plt.axhline(y=1, xmin=0, xmax=1, color='tomato', linewidth=2.5)\n",
    "# Annotate the raw value of the baseline\n",
    "for idx, value in enumerate(order):\n",
    "    bm = base_means[f\"micro.math-{value}\"]\n",
    "    if bm > 1000000:\n",
    "        v = f\"{bm / 1000000:.0f}M\"\n",
    "    elif bm > 1000:\n",
    "        v = f\"{bm / 1000:.1f}k\"\n",
    "    else:\n",
    "        v = f\"{bm:.0f}\"\n",
    "    #tmp = max(mean_df.loc[mean_df['bench'] == value].values[0][1:]) / base_means[value]\n",
    "    tmp = max(df_norm.loc[df_norm['bench'] == value]['norm'].values)\n",
    "    plt.text(idx, tmp+.5, f\"{v}\", fontsize='xx-small', color='tomato', ha='center')\n",
    "#plt.vlines(range(0, len(order)), ymin=0, ymax=max_val, linestyle='dashed', colors='grey', linewidth=.25, zorder=0)\n",
    "\n",
    "# Set color + hatch\n",
    "style = {\n",
    "    'fill': [ True, True ],\n",
    "    'color': [ palette['bluish green'], palette['orange'] ],\n",
    "    'hatch': [ '', '', ''],\n",
    "    'edgecolor': [ 'black', 'black', 'black' ]\n",
    "}\n",
    "for idx, bar in enumerate(ax.patches):\n",
    "    bar_nr = int(idx / int(len(order)))\n",
    "    bar.set(color=style['color'][bar_nr], fill=style['fill'][bar_nr],\n",
    "            hatch=style['hatch'][bar_nr], edgecolor=style['edgecolor'][bar_nr])\n",
    "\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(.5, 1.15), borderaxespad=0, ncol=2, fontsize='x-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41550396",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(f\"{input_csv[:-3]}pdf\", dpi=500, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd6f91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sorted_nicely(base_means):\n",
    "    print(f\"{i:25}: {base_means[i]:>20.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8062f960",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"bench{20*' '}    risotto     native\")\n",
    "for b in sorted_nicely(base_means):\n",
    "    ris = df_norm.loc[(df_norm['bench'] == b[11:]) & (df_norm['tag'] == 'risotto')]\n",
    "    nat = df_norm.loc[(df_norm['bench'] == b[11:]) & (df_norm['tag'] == 'native')]\n",
    "    print(f\"{b:25} {np.mean(ris['norm'].values):>10.4f} {np.mean(nat['norm'].values):>10.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84217928",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
